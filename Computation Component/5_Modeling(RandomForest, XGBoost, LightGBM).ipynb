{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8218cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score,\n",
    "    accuracy_score, precision_score, recall_score, f1_score\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dbf465c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR       = Path(\"data\")\n",
    "NPZ_PATH       = DATA_DIR / \"part4_Xy_arrays.npz\"\n",
    "FEATS_JSON     = DATA_DIR / \"part4_feature_names.json\"\n",
    "\n",
    "BEST_MODEL_PATH   = DATA_DIR / \"part5_best_model.pkl\"\n",
    "SUMMARY_CSV_PATH  = DATA_DIR / \"part5_model_summary.csv\"\n",
    "TEST_PRED_PATH    = DATA_DIR / \"part5_test_predictions.csv\"\n",
    "META_JSON_PATH    = DATA_DIR / \"part5_best_model_meta.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e299e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert NPZ_PATH.exists(), f\"Missing {NPZ_PATH}\"\n",
    "assert FEATS_JSON.exists(), f\"Missing {FEATS_JSON}\"\n",
    "\n",
    "arrs = np.load(NPZ_PATH)\n",
    "X_train, y_train = arrs[\"X_train\"], arrs[\"y_train\"]\n",
    "X_test,  y_test  = arrs[\"X_test\"],  arrs[\"y_test\"]\n",
    "\n",
    "with open(FEATS_JSON, \"r\") as f:\n",
    "    FEATS_META = json.load(f)\n",
    "FEATURE_NAMES = FEATS_META[\"FEATURE_NAMES\"]\n",
    "LABEL_COL     = FEATS_META[\"LABEL_COL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7061bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:\n",
      "  X_train: (588899, 29) y_train: (588899,)\n",
      "  X_test : (147225, 29) y_test : (147225,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shapes:\")\n",
    "print(\"  X_train:\", X_train.shape, \"y_train:\", y_train.shape)\n",
    "print(\"  X_test :\", X_test.shape,  \"y_test :\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "344d3f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------- 5.1 (Optional) Training subsample for faster prototyping ----------\n",
    "# Set TRAIN_SUBSAMPLE = None to use ALL rows; or set an integer (e.g., 200_000)\n",
    "TRAIN_SUBSAMPLE = None  # e.g., 200_000\n",
    "\n",
    "if (TRAIN_SUBSAMPLE is not None) and (TRAIN_SUBSAMPLE < len(y_train)):\n",
    "    X_train_sub, _, y_train_sub, _ = train_test_split(\n",
    "        X_train, y_train,\n",
    "        train_size=TRAIN_SUBSAMPLE,\n",
    "        random_state=42,\n",
    "        stratify=y_train\n",
    "    )\n",
    "else:\n",
    "    X_train_sub, y_train_sub = X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d491800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a small validation split from training for early stopping & threshold tuning\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_train_sub, y_train_sub,\n",
    "    test_size=0.10, random_state=42, stratify=y_train_sub\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d5c2afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 5.2 Define models ----------\n",
    "# 1) RandomForest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=None,\n",
    "    min_samples_leaf=1,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ccf855d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) XGBoost\n",
    "xgb_ok = True\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    xgb = XGBClassifier(\n",
    "        n_estimators=600,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_lambda=1.0,\n",
    "        tree_method=\"hist\",\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        eval_metric=\"auc\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    xgb_ok = False\n",
    "    xgb_err = str(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7175039f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) LightGBM\n",
    "lgbm_ok = True\n",
    "try:\n",
    "    from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
    "    lgbm = LGBMClassifier(\n",
    "        n_estimators=800,\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=31,\n",
    "        feature_fraction=0.8,\n",
    "        bagging_fraction=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "except Exception as e:\n",
    "    lgbm_ok = False\n",
    "    lgbm_err = str(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c22b01a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 5.3 Train each model (using train/val split) ----------\n",
    "results = []\n",
    "fitted = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5068f9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to compute metrics\n",
    "def compute_metrics(name, y_true, proba, thr=0.5):\n",
    "    pred = (proba >= thr).astype(int)\n",
    "    return {\n",
    "        \"model\": name,\n",
    "        \"roc_auc\": float(roc_auc_score(y_true, proba)),\n",
    "        \"pr_auc\": float(average_precision_score(y_true, proba)),\n",
    "        \"accuracy@0.5\": float(accuracy_score(y_true, pred)),\n",
    "        \"precision@0.5\": float(precision_score(y_true, pred, zero_division=0)),\n",
    "        \"recall@0.5\": float(recall_score(y_true, pred, zero_division=0)),\n",
    "        \"f1@0.5\": float(f1_score(y_true, pred, zero_division=0)),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "da6707b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_f1_threshold(y_true, proba, grid=None):\n",
    "    if grid is None:\n",
    "        grid = np.linspace(0.05, 0.95, 19)\n",
    "    scores = [(t, f1_score(y_true, (proba >= t).astype(int), zero_division=0)) for t in grid]\n",
    "    best_t, best_f1 = max(scores, key=lambda x: x[1])\n",
    "    return float(best_t), float(best_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "02c4251b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- RandomForest ---\n",
    "rf.fit(X_tr, y_tr)\n",
    "rf_val_proba = rf.predict_proba(X_val)[:, 1]\n",
    "rf_metrics_val = compute_metrics(\"RandomForest (val)\", y_val, rf_val_proba)\n",
    "results.append(rf_metrics_val)\n",
    "fitted[\"RandomForest\"] = rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7a7c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- XGBoost ---\n",
    "if xgb_ok:\n",
    "    xgb.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        verbose=False,\n",
    "        early_stopping_rounds=50\n",
    "    )\n",
    "    xgb_val_proba = xgb.predict_proba(X_val)[:, 1]\n",
    "    xgb_metrics_val = compute_metrics(\"XGBoost (val)\", y_val, xgb_val_proba)\n",
    "    results.append(xgb_metrics_val)\n",
    "    fitted[\"XGBoost\"] = xgb\n",
    "else:\n",
    "    print(\"XGBoost not available:\", xgb_err)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1cf3041d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 115925, number of negative: 414084\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002365 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 948\n",
      "[LightGBM] [Info] Number of data points in the train set: 530009, number of used features: 29\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.218723 -> initscore=-1.273125\n",
      "[LightGBM] [Info] Start training from score -1.273125\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n"
     ]
    }
   ],
   "source": [
    "# --- LightGBM ---\n",
    "if lgbm_ok:\n",
    "    lgbm.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        callbacks=[early_stopping(50, verbose=False)]\n",
    "    )\n",
    "    lgbm_val_proba = lgbm.predict_proba(X_val)[:, 1]\n",
    "    lgbm_metrics_val = compute_metrics(\"LightGBM (val)\", y_val, lgbm_val_proba)\n",
    "    results.append(lgbm_metrics_val)\n",
    "    fitted[\"LightGBM\"] = lgbm\n",
    "else:\n",
    "    print(\"LightGBM not available:\", lgbm_err)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c8767956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Validation summary (higher ROC-AUC is better) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>pr_auc</th>\n",
       "      <th>accuracy@0.5</th>\n",
       "      <th>precision@0.5</th>\n",
       "      <th>recall@0.5</th>\n",
       "      <th>f1@0.5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LightGBM (val)</td>\n",
       "      <td>0.660899</td>\n",
       "      <td>0.345702</td>\n",
       "      <td>0.781678</td>\n",
       "      <td>0.532432</td>\n",
       "      <td>0.015294</td>\n",
       "      <td>0.029734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RandomForest (val)</td>\n",
       "      <td>0.587400</td>\n",
       "      <td>0.273594</td>\n",
       "      <td>0.732790</td>\n",
       "      <td>0.305173</td>\n",
       "      <td>0.173589</td>\n",
       "      <td>0.221298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model   roc_auc    pr_auc  accuracy@0.5  precision@0.5  \\\n",
       "0      LightGBM (val)  0.660899  0.345702      0.781678       0.532432   \n",
       "1  RandomForest (val)  0.587400  0.273594      0.732790       0.305173   \n",
       "\n",
       "   recall@0.5    f1@0.5  \n",
       "0    0.015294  0.029734  \n",
       "1    0.173589  0.221298  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary_val = pd.DataFrame(results).sort_values(\"roc_auc\", ascending=False).reset_index(drop=True)\n",
    "print(\"\\n=== Validation summary (higher ROC-AUC is better) ===\")\n",
    "display(summary_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c55a24f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best (val): LightGBM\n"
     ]
    }
   ],
   "source": [
    "# ---------- 5.4 Pick best by validation ROC-AUC & refit on FULL TRAIN ----------\n",
    "# Choose best name among keys present in `fitted`\n",
    "def _name_from_row(row_model):\n",
    "    # row_model like \"RandomForest (val)\" -> \"RandomForest\"\n",
    "    return row_model.split(\" \", 1)[0]\n",
    "\n",
    "best_row = summary_val.iloc[0]\n",
    "BEST_MODEL_NAME = _name_from_row(best_row[\"model\"])\n",
    "print(\"\\nBest (val):\", BEST_MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6f423f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 115925, number of negative: 414084\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003501 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 948\n",
      "[LightGBM] [Info] Number of data points in the train set: 530009, number of used features: 29\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.218723 -> initscore=-1.273125\n",
      "[LightGBM] [Info] Start training from score -1.273125\n"
     ]
    }
   ],
   "source": [
    "# Refit best model on ALL of X_train (with 10% as internal val if boosting)\n",
    "if BEST_MODEL_NAME == \"RandomForest\":\n",
    "    best_model = RandomForestClassifier(\n",
    "        n_estimators=300, max_depth=None, min_samples_leaf=1,\n",
    "        n_jobs=-1, random_state=42\n",
    "    ).fit(X_train, y_train)\n",
    "\n",
    "elif BEST_MODEL_NAME == \"XGBoost\":\n",
    "    best_model = XGBClassifier(\n",
    "        n_estimators=600, learning_rate=0.05, max_depth=6,\n",
    "        subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0,\n",
    "        tree_method=\"hist\", random_state=42, n_jobs=-1, eval_metric=\"auc\"\n",
    "    )\n",
    "    X_tr_full, X_val_full, y_tr_full, y_val_full = train_test_split(\n",
    "        X_train, y_train, test_size=0.1, random_state=42, stratify=y_train\n",
    "    )\n",
    "    best_model.fit(\n",
    "        X_tr_full, y_tr_full,\n",
    "        eval_set=[(X_val_full, y_val_full)],\n",
    "        verbose=False,\n",
    "        early_stopping_rounds=50\n",
    "    )\n",
    "\n",
    "elif BEST_MODEL_NAME == \"LightGBM\":\n",
    "    best_model = LGBMClassifier(\n",
    "        n_estimators=800, learning_rate=0.05, num_leaves=31,\n",
    "        feature_fraction=0.8, bagging_fraction=0.8,\n",
    "        random_state=42, n_jobs=-1\n",
    "    )\n",
    "    X_tr_full, X_val_full, y_tr_full, y_val_full = train_test_split(\n",
    "        X_train, y_train, test_size=0.1, random_state=42, stratify=y_train\n",
    "    )\n",
    "    best_model.fit(\n",
    "        X_tr_full, y_tr_full,\n",
    "        eval_set=[(X_val_full, y_val_full)],\n",
    "        callbacks=[early_stopping(50, verbose=False)]\n",
    "    )\n",
    "\n",
    "else:\n",
    "    raise RuntimeError(\"Unknown BEST_MODEL_NAME.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9db069f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n"
     ]
    }
   ],
   "source": [
    "# ---------- 5.5 Evaluate on TEST ----------\n",
    "proba_test = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Default-threshold metrics\n",
    "metrics_test = compute_metrics(BEST_MODEL_NAME + \" (test)\", y_test, proba_test, thr=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d6a7bacc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n"
     ]
    }
   ],
   "source": [
    "# F1-tuned threshold (on training probabilities)\n",
    "proba_train = best_model.predict_proba(X_train)[:, 1]\n",
    "best_t, best_f1_train = find_best_f1_threshold(y_train, proba_train)\n",
    "pred_test_tuned = (proba_test >= best_t).astype(int)\n",
    "\n",
    "metrics_test_tuned = {\n",
    "    \"model\": BEST_MODEL_NAME + \" (test, tuned)\",\n",
    "    \"threshold*\": best_t,\n",
    "    \"accuracy@t\": float(accuracy_score(y_test, pred_test_tuned)),\n",
    "    \"precision@t\": float(precision_score(y_test, pred_test_tuned, zero_division=0)),\n",
    "    \"recall@t\": float(recall_score(y_test, pred_test_tuned, zero_division=0)),\n",
    "    \"f1@t\": float(f1_score(y_test, pred_test_tuned, zero_division=0)),\n",
    "    \"roc_auc\": float(roc_auc_score(y_test, proba_test)),\n",
    "    \"pr_auc\": float(average_precision_score(y_test, proba_test))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "028745ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Test metrics (threshold=0.5) ===\n",
      "{'model': 'LightGBM (test)', 'roc_auc': 0.6396863299011186, 'pr_auc': 0.3259998686288026, 'accuracy@0.5': 0.7812124299541519, 'precision@0.5': 0.49476987447698745, 'recall@0.5': 0.014688984814136207, 'f1@0.5': 0.02853092861235938}\n",
      "\n",
      "=== Test metrics (tuned threshold) ===\n",
      "{'model': 'LightGBM (test, tuned)', 'threshold*': 0.2, 'accuracy@t': 0.5533503141450161, 'precision@t': 0.28230943885825494, 'recall@t': 0.6757243563864477, 'f1@t': 0.3982393206193492, 'roc_auc': 0.6396863299011186, 'pr_auc': 0.3259998686288026}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Test metrics (threshold=0.5) ===\")\n",
    "print(metrics_test)\n",
    "print(\"\\n=== Test metrics (tuned threshold) ===\")\n",
    "print(metrics_test_tuned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "df609d6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/part5_best_model.pkl']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------- 5.6 Save artifacts ----------\n",
    "# 1) Best model\n",
    "joblib.dump(best_model, BEST_MODEL_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a9ac0d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Model comparison summary (validation metrics + test metrics for best)\n",
    "summary_val.to_csv(SUMMARY_CSV_PATH, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c727c81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Test predictions CSV (for downstream dashboards or error analysis)\n",
    "test_pred_df = pd.DataFrame({\n",
    "    \"y_test\": y_test,\n",
    "    \"p_delay_best\": proba_test,\n",
    "    \"pred_0_5\": (proba_test >= 0.5).astype(int),\n",
    "    \"pred_tuned\": pred_test_tuned\n",
    "})\n",
    "test_pred_df.to_csv(TEST_PRED_PATH, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "252a38b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Metadata about the run\n",
    "META = {\n",
    "    \"best_model_name\": BEST_MODEL_NAME,\n",
    "    \"feature_names\": FEATURE_NAMES,\n",
    "    \"threshold_tuned\": best_t,\n",
    "    \"train_subsample\": TRAIN_SUBSAMPLE,\n",
    "    \"paths\": {\n",
    "        \"best_model_path\": str(BEST_MODEL_PATH),\n",
    "        \"summary_csv\": str(SUMMARY_CSV_PATH),\n",
    "        \"test_predictions\": str(TEST_PRED_PATH)\n",
    "    },\n",
    "    \"test_metrics@0.5\": metrics_test,\n",
    "    \"test_metrics@tuned\": metrics_test_tuned\n",
    "}\n",
    "with open(META_JSON_PATH, \"w\") as f:\n",
    "    json.dump(META, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4dfb850d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved artifacts:\n",
      "  Best model: data/part5_best_model.pkl\n",
      "  Comparison: data/part5_model_summary.csv\n",
      "  Test preds: data/part5_test_predictions.csv\n",
      "  Meta JSON : data/part5_best_model_meta.json\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSaved artifacts:\")\n",
    "print(\"  Best model:\", BEST_MODEL_PATH)\n",
    "print(\"  Comparison:\", SUMMARY_CSV_PATH)\n",
    "print(\"  Test preds:\", TEST_PRED_PATH)\n",
    "print(\"  Meta JSON :\", META_JSON_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665b430f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2017c85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
